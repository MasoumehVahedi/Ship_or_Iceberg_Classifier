# -*- coding: utf-8 -*-
"""FCN-Tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yK57-GV-8R9i5W546A7BgK7RMSUxvZmm
"""

# Import the necessary libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import itertools
import os

from keras.layers import Input  
from keras.layers import Activation
from keras.layers import Conv2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import MaxPooling2D
from keras.layers import BatchNormalization
from keras.layers import Dropout
from keras.models import Sequential
from keras.callbacks import ModelCheckpoint
from keras.callbacks import EarlyStopping
from keras.callbacks import TensorBoard

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix

"""**Load the dataset and data exploratory**"""

#Load dataset
df_train = pd.read_json("/content/drive/MyDrive/train.json")
df_train.head()

print(df_train.shape)

print(df_train.info())

# MISSING VALUES:
#NOTE: we want to find "Nan" filds and replace it
df_train.inc_angle.replace({"na":np.nan}, inplace=True)
# Drop the rows that has NaN value for inc_angle
df_train.drop(df_train[df_train["inc_angle"].isnull()].index, inplace=True)

sns.countplot(df_train["is_iceberg"])

df_train["is_iceberg"].value_counts()

iceberg_size = df_train["is_iceberg"].value_counts(sort=1)
plt.pie(iceberg_size, autopct = "%1.1f%%")
plt.show()

sns.distplot(df_train["is_iceberg"])

sns.countplot(df_train["is_iceberg"])

df_train["is_iceberg"].value_counts()

iceberg_size = df_train["is_iceberg"].value_counts(sort=1)
plt.pie(iceberg_size, autopct = "%1.1f%%")
plt.show()

sns.distplot(df_train["is_iceberg"])

def prepare_data(df):
    X_band_1 = []
    X_band_2 = []
    
    for band in df["band_1"]:
        #Convert to float32
        band_1 = np.array(band).astype(np.float32)
        #Reshaping band_1 and band_2
        band_1 = band_1.reshape(75,75)
        X_band_1.append(band_1)
        
    for band in df["band_2"]:
         #Convert to float32
        band_2 = np.array(band).astype(np.float32)
        #Reshaping band_1 and band_2
        band_2 = band_2.reshape(75,75)
        X_band_2.append(band_2)
        
    #Convert list to numpy array
    X_band_1 = np.array(X_band_1)
    X_band_2 = np.array(X_band_2)
    
    # Rescale
    X_band_1 = (X_band_1 - X_band_1.mean()) / (X_band_1.max() - X_band_1.min())
    X_band_2 = (X_band_2 - X_band_2.mean()) / (X_band_2.max() - X_band_2.min())
    
    #Concatenate band_1 and band_2 to create X for training (or test)
    X = np.concatenate([X_band_1[:, :, :, np.newaxis], 
                        X_band_2[:, :, :, np.newaxis],((X_band_1+X_band_2)/2)[:, :, :, np.newaxis]], 
                        axis=-1)
    
    Y = np.array(df["is_iceberg"])
    
    return X, Y

# Load X and Y
X, Y = prepare_data(df_train)

print("X shape is:{}".format(X.shape))
print("Y shape is:{}".format(Y.shape))


#Split data to test and train
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print("X_train shape = {}".format(X_train.shape))
print("X_test shape = {}".format(X_test.shape))
print("Y_train shape = {}".format(Y_train.shape))
print("Y_test shape = {}".format(Y_test.shape))

"""**CNN Tensorflow**"""

# number of classes
k = len(set(Y_train))
print("Number of classes: {}".format(k))

def FCN_model(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS):
    
    inputs = (IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)
    
    # Build the model
    model = Sequential()
    
    # Conv 1
    model.add(Conv2D(64, kernel_size=(3,3), input_shape=inputs))
    model.add(Activation("relu"))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Dropout(0.2))
    
    # Conv 2
    model.add(Conv2D(128, kernel_size=(3,3)))
    model.add(Activation("relu"))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Dropout(0.2))
    
    # Conv 3
    model.add(Conv2D(128, kernel_size=(3,3)))
    model.add(Activation("relu"))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Dropout(0.2))
    
    # Conv 4
    model.add(Conv2D(64, kernel_size=(3,3)))
    model.add(Activation("relu"))
    model.add(MaxPooling2D(pool_size=(2,2)))
    model.add(Dropout(0.2))
    
    # Flatten layer
    model.add(Flatten())
    
    # Dense layer 1
    model.add(Dense(512))
    model.add(Activation("relu"))
    model.add(Dropout(0.2))
    
    # Dense layer 2
    model.add(Dense(256))
    model.add(Activation("relu"))
    model.add(Dropout(0.2))
    
    # Output layer
    model.add(Dense(1))
    model.add(Activation("sigmoid"))
    
    # Compile the model
    model.compile(loss = "binary_crossentropy", 
                  optimizer = "adam",
                  metrics = ["accuracy"])
    model.summary()
    
    return model

# Build FCN model
IMAGE_WIDTH = 75
IMAGE_HEIGHT = 75
IMAGE_CHANNELS = 3
batch_size = 32

fcn_model = FCN_model(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)

# Fit the model
history = fcn_model.fit(X_train, Y_train, 
                        validation_data = (X_test, Y_test), 
                        epochs = 10, 
                        batch_size = batch_size,
                        shuffle = True)

loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
plt.plot(epochs, acc, 'y', label='Training accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Evaluate model
test_loss, test_acc = fcn_model.evaluate(X_test, Y_test, verbose=1)
print("Test Score = ", test_loss)
print("Test Accuracy = ", test_acc)

"""**Model Metrics**

Three metrics to evaluate the classification accuracy of neural network, which are being used commonly, incldue:

1- Precision

2- Recall

3- F1 Score
"""

# Evaluate augmnented model result

# Predict the probability
y_p = fcn_model.predict(X_test, verbose=0).argmax(axis=1)

# Predict each class
y_classes = fcn_model.predict_classes(X_test, verbose=0)

from sklearn.metrics import classification_report
print(classification_report(Y_test, y_p))

def cal_accuracy(predictions, y_classes):
    
    # precision tp / (tp + fp)
    # 'average' parameter is required for multiclass/multilabel targets
    precision = precision_score(predictions, y_classes, average='weighted')
    print('Precision score is : {}'.format(precision))
    
    # recall: tp / (tp + fn)
    recall = recall_score(predictions, y_classes, average='weighted')
    print('Recall score is : {}'.format(recall))
    
    # f1: 2 tp / (2 tp + fp + fn)
    f1 = f1_score(predictions, y_classes, average='weighted')
    print('f1 score is : {}'.format(f1))
    
    return precision, recall, f1

metrics = cal_accuracy(y_p, y_classes) 
print(metrics)